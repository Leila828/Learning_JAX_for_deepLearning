{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFMYZK3kRNT6Ogo+pr4KH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leila828/Learning_JAX_for_deepLearning/blob/main/jax_Linear_Algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vectors**"
      ],
      "metadata": {
        "id": "GEm-wOpKLPtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are assuming you are fairly familiar with NumPy, this chapter is by no means a thorough coverage of the topic and will just serve as a quick refresher. We will practice using JAX while discovering the equivalence between normal NumPy and the JAX version of NumPy’s syntax.\n",
        "\n",
        "Note: As a reference, we’ll use np and jnp respectively for default and JAX NumPy versions in our codes."
      ],
      "metadata": {
        "id": "tTshRbT_LdDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inner product#**\n",
        "The inner product of two vectors can be calculated by any of the three syntaxes (dot(), inner() and @), as shown below:"
      ],
      "metadata": {
        "id": "-PccD7GaLeBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myVdQ2p-K9dX"
      },
      "outputs": [],
      "source": [
        "a = jnp.arange(1,10)\n",
        "b = jnp.arange(11,20)\n",
        "\n",
        "print(jnp.dot(a,b))\n",
        "print(jnp.inner(a,b))\n",
        "print(a@b)\n",
        "print(b@a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear functions#\n",
        "A linear function can be represented as:\n",
        "\n",
        "f(x) = \\alpha x_1+\\beta x_2+\\gamma x_3+....\n",
        "f(x)=αx \n",
        "1\n",
        "​\n",
        " +βx \n",
        "2\n",
        "​\n",
        " +γx \n",
        "3\n",
        "​\n",
        " +....\n",
        "\n",
        "We can compose it as a lambda function too:"
      ],
      "metadata": {
        "id": "l9a2Gvb1LjM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a= 1.2\n",
        "b = 2.4\n",
        "\n",
        "FuncA = lambda x : a*x[0]+b*x[1]\n",
        "\n",
        "x = jnp.arange(1,3)\n",
        "print(FuncA(x))"
      ],
      "metadata": {
        "id": "gUo0VRzILmaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Taylor approximation#**\n",
        "Taylor approximation lies at the core of major optimization algorithms in machine learning. We can first-order approximate the value of a function near a point z as:\n",
        "\n",
        "\\hat{f}(x) = f(z) +\\nabla f(z)^T (x-z)\n",
        "f\n",
        "^\n",
        "​\n",
        " (x)=f(z)+∇f(z) \n",
        "T\n",
        " (x−z)\n",
        "\n",
        "Having already seen the efficiency of grad(), we can easily implement the Taylor approximation. For example, if at z = (1,-1) we want to approximate the value of the following function:\n",
        "\n",
        "f(x) = 2x_1+3x_2\n",
        "f(x)=2x \n",
        "1\n",
        "​\n",
        " +3x \n",
        "2\n",
        "​\n"
      ],
      "metadata": {
        "id": "Mj_OoWDsLs4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FuncA = lambda x : 2*x[0]+3*x[1]\n",
        "Gradient = lambda x : grad(FuncA)(x)\n",
        "TaylorApprox = lambda x:FuncA(z)+Gradient(z)@(x-z) \n",
        "\n",
        "z = jnp.array([1.0,-1.0])\n",
        "print(\"Actual function value at z is:\",FuncA(z))\n",
        "print(\"Taylor Approximation at same value is:\",TaylorApprox(z))"
      ],
      "metadata": {
        "id": "yQ_owgwGLz2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Norms#\n",
        "The Euclidean norm of any vector is defined as:\n",
        "\n",
        "||x|| = \\sqrt {x_1^2 + x_2^2 +....+x_n^2}\n",
        "∣∣x∣∣= \n",
        "x \n",
        "1\n",
        "2\n",
        "​\n",
        " +x \n",
        "2\n",
        "2\n",
        "​\n",
        " +....+x \n",
        "n\n",
        "2\n",
        "​\n",
        " \n",
        "​\n",
        " \n",
        "\n",
        "Norms can be calculated using the package linalg. We can also use norms to find the Euclidean distance between two vectors:"
      ],
      "metadata": {
        "id": "g55u1RdfLwna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(1,100)\n",
        "y = jnp.arange(11,110)\n",
        "x_norm = jnp.linalg.norm(x)\n",
        "\n",
        "print(\"Norm of x is: \",x_norm)\n",
        "print(\"Distance between x and y vectors is:\",jnp.linalg.norm(x-y))"
      ],
      "metadata": {
        "id": "L4BSzD-mL4dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity#\n",
        "A common way of finding the similarity between two vectors in machine learning, especially in NLP applications, is to calculate the cosine similarity. Using the definition of the dot product:\n",
        "\n",
        "x.y = x^Ty = ||x||.||y|| cos\\theta\n",
        "x.y=x \n",
        "T\n",
        " y=∣∣x∣∣.∣∣y∣∣cosθ\n",
        "\n",
        "We can then calculate the cosine similarity:\n",
        "\n",
        "cos \\theta = \\frac {x^Ty}{||x||.||y||}\n",
        "cosθ= \n",
        "∣∣x∣∣.∣∣y∣∣\n",
        "x \n",
        "T\n",
        " y\n",
        "​\n"
      ],
      "metadata": {
        "id": "8V0dQ380MIuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(1,100)\n",
        "y = jnp.arange(11,110)\n",
        "\n",
        "cosine = lambda x,y: (x@y)/(jnp.linalg.norm(x)*jnp.linalg.norm(y))\n",
        "print(\"Cosine similarity between x and y is:\",cosine(x,y))"
      ],
      "metadata": {
        "id": "0ygGJU0rMLst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard deviation#\n",
        "The standard deviation of a vector is defined, where n\n",
        "n\n",
        " is the size of vector, as:\n",
        "\n",
        "std(x) = \\frac{||x-\\bar x||}{\\sqrt{n}}\n",
        "std(x)= \n",
        "n\n",
        "​\n",
        " \n",
        "∣∣x− \n",
        "x\n",
        "ˉ\n",
        " ∣∣\n",
        "​\n",
        " \n",
        "\n",
        "We can directly calculate this using jnp.std(). The following example shows the use of both direct std() and implementation using the above formula:"
      ],
      "metadata": {
        "id": "VqrHtXvxMNo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(20,61)\n",
        "y = jnp.arange(1,101)\n",
        "\n",
        "stdv = lambda x: jnp.linalg.norm(x-jnp.average(x))/(jnp.sqrt(x.size))\n",
        "\n",
        "print(\"SD of x using our function is: \",stdv(x))\n",
        "print(\"SD of y using our function is:\",stdv(y))\n",
        "#Answers will be same\n",
        "print(\"Actual SD of x is: \",jnp.std(x))\n",
        "print(\"Actual SD of y is:\",jnp.std(y))"
      ],
      "metadata": {
        "id": "dzINEDlXMQzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrices**"
      ],
      "metadata": {
        "id": "THKudxZJn0KE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrices are at the core of almost any data application. Even vectors can be treated as a matrix with either a row or column dimension of 1.\n",
        "\n",
        "Note: We will use the common notation of capital letters of linear algebra for matrices in our codes as well."
      ],
      "metadata": {
        "id": "VUjOMLe0n3aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.arange(5)\n",
        "b = 2*jnp.arange(5)\n",
        "c = -1*jnp.arange(5)\n",
        "\n",
        "A = jnp.array((a,b,c)) #Concatenation of vectors to make a matrix\n",
        "\n",
        "print(A)\n",
        "print(A.shape)"
      ],
      "metadata": {
        "id": "tmngBcYNn4Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Slicing#**\n",
        "We can make submatrices by using the slicing (:) notation.\n",
        "\n",
        "Note: Python uses 0-based indexing, as does JAX.\n",
        "\n",
        "For example, as shown in line 10 below, a submatrix containing the first 2 rows and first 3 columns of the above matrix will be:\n",
        "\n",
        "B = A[0:2,0:3]\n",
        "\n",
        "**Reshaping functions#**\n",
        "Reshaping functions are commonly used in several applications, especially computer vision.\n",
        "\n",
        "The rule behind any reshaping function is simple: If the input and output matrices have mxn and jxk dimensions respectively, then:\n",
        "\n",
        "m\\times n = j\\times k\n",
        "m×n=j×k\n",
        "\n",
        "We can reshape a matrix A of dimensions (m,n) into B as:\n",
        "\n",
        "B = A.reshape((j,k))\n",
        "\n",
        "Note: The reshape() operator is inconsistent with linear algebra and is just a useful feature in NumPy and JAX programming.\n",
        "\n",
        "**Stacking#**\n",
        "We can directly stack a collection of vectors:\n",
        "\n",
        "Vertically (for row vectors): Using vstack() or r_[]\n",
        "Horizontally (for column vectors): Using hstack() or c_[]"
      ],
      "metadata": {
        "id": "UJtyfv1Zn6SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.arange(5)\n",
        "b = 2*jnp.arange(5)\n",
        "c = -1*jnp.arange(5)\n",
        "\n",
        "A = jnp.array((a,b,c)) \n",
        "\n",
        "print(A)\n",
        "print(A.shape)\n",
        "\n",
        "B = A[0:2,0:3]\n",
        "\n",
        "print(B)\n",
        "\n",
        "C = A[1:3,1:4]\n",
        "print(C)\n",
        "\n",
        "D = jnp.vstack([B,C])\n",
        "print(D)\n",
        "\n",
        "E = jnp.r_[B,C]\n",
        "print(E) #Same as D"
      ],
      "metadata": {
        "id": "ge2x1HdNoCTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiplication#**\n",
        "Matrices are multiplied by the usual linear algebra rules using either @ or matmul():"
      ],
      "metadata": {
        "id": "GVypNq1MoEDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.arange(5)\n",
        "b = 2*jnp.arange(5)\n",
        "c = -1*jnp.arange(5)\n",
        "\n",
        "A = jnp.array((a,b,c)) \n",
        "\n",
        "print(\"A and its shape\")\n",
        "print(A)\n",
        "print(A.shape)\n",
        "\n",
        "B = A@A.T #Multiplying A with its transpose\n",
        "\n",
        "print(\"B = A@A and its shape\")\n",
        "print(B)\n",
        "print(B.shape)\n",
        "\n",
        "C= jnp.matmul(A,A.T) #matmul() yields the same answer.\n",
        "\n",
        "print(\"C will have same contents as B and its shape\")\n",
        "print(C)\n",
        "print(C.shape)"
      ],
      "metadata": {
        "id": "Vl6AUlGIof1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random matrices#**\n",
        "There will often be scenarios where we need to sample a matrix stochastically. This will come in handy in many real-life applications.\n",
        "\n",
        "One of the core features of JAX is its PRNG. To sample a matrix from a given distribution, we need to specify the PRNG key first, followed by its shape. For example, we can sample a 4x4 matrix as:"
      ],
      "metadata": {
        "id": "TBnTQ42iolrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jax.random.uniform(key,(4,4))"
      ],
      "metadata": {
        "id": "PdttaboXonB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The next chapter clarifies how to get detailed coverage by providing insight into both JAX’s PNRG and probability distributions."
      ],
      "metadata": {
        "id": "to7TjDMCoo1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Norm#\n",
        "A matrix norm, also known as a Frobenius norm, is defined as:\n",
        "\n",
        "||A|| = \\sqrt {(\\sum_{i=1}^m\\sum_{j=1}^n A_{ij}^2)}\n",
        "∣∣A∣∣= \n",
        "( \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " A \n",
        "ij\n",
        "2\n",
        "​\n",
        " )\n",
        "​\n",
        " \n",
        "\n",
        "We can use the same linalg.norm() for the Frobenius norm as well."
      ],
      "metadata": {
        "id": "wZmr9y2-otIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "from jax.numpy import linalg\n",
        "\n",
        "a = jnp.arange(5)\n",
        "b = 2*jnp.arange(5)\n",
        "c = -1*jnp.arange(5)\n",
        "\n",
        "A = jnp.array((a,b,c))\n",
        "\n",
        "print(\"A and its shape\")\n",
        "print(A)\n",
        "print(A.shape)\n",
        "\n",
        "A_norm = linalg.norm(A)\n",
        "print(\"A's norm is: \",A_norm)\n",
        "\n",
        "key = random.PRNGKey(0) #Don't worry about it. Next chapter covers it in depth\n",
        "B = random.normal(key,(100,100)) \n",
        "\n",
        "B_norm = linalg.norm(B)\n",
        "print(\"B's norm is:\", B_norm)"
      ],
      "metadata": {
        "id": "XJWbi3E2orKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "note\n",
        "While it’s correct to refer to the Frobenius norm as Euclidean norm, the L2 norm for a matrix is instead defined as:\n",
        "\n",
        "||X||_2 = \\sigma_{max}(X)=\\sqrt{\\lambda_{max}(X^TX)}\n",
        "∣∣X∣∣ \n",
        "2\n",
        "​\n",
        " =σ \n",
        "max\n",
        "​\n",
        " (X)= \n",
        "λ \n",
        "max\n",
        "​\n",
        " (X \n",
        "T\n",
        " X)\n",
        "​\n",
        " \n",
        "\n",
        "It is used less frequently than the Frobenius norm."
      ],
      "metadata": {
        "id": "GDXMrtoEo1fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More on Matrices**"
      ],
      "metadata": {
        "id": "vbwc8RHWOyPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the vast world of matrices and the implementation is more than even a dedicated course can cover, we’ll try to do some justice to the subject by concluding with another lesson covering a few important concepts.\n",
        "\n",
        "Determinant#\n",
        "Like the norm, the determinant is a scalar value characterizing a square matrix. It is quite useful in defining some properties of the matrix.\n",
        "\n",
        "For example, a matrix is singular if (and only if) its determinant is zero.\n",
        "\n",
        "We can calculate it as:"
      ],
      "metadata": {
        "id": "LquGjR02O0Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = linalg.det(A)"
      ],
      "metadata": {
        "id": "mYO4J3QTO33O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inverse#**\n",
        "The inverse of a matrix is defined as:\n",
        "\n",
        "A^{-1} = \\frac{Adj. A}{|A|}\n",
        "A \n",
        "−1\n",
        " = \n",
        "∣A∣\n",
        "Adj.A\n",
        "​\n",
        " \n",
        "\n",
        "The linalg sublibrary provides the function for inverse calculation as:\n",
        "\n",
        "A_inv = linalg.inv(A)\n",
        "Note: The linalg.pinv() can be used to calculate a pseudo-inverse and doesn’t support the int members.\n",
        "\n",
        "Power# **bold text**\n",
        "Similarly, we can calculate the power of A using matrix_power() as:\n",
        "\n",
        "A_square = linalg.matrix_power(A,2)\n"
      ],
      "metadata": {
        "id": "TDGyUDGxO6pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 2, 3],[ 4, 5, 6],[7, 8, 9]])\n",
        "print(A)\n",
        "\n",
        "B = linalg.matrix_power(A,-1)\n",
        "C = linalg.inv(A)\n",
        "d = linalg.det(A)\n",
        "\n",
        "print(B)\n",
        "print(C) # will have same answer as B\n",
        "print(\"Matrix's determinant is: \",d)"
      ],
      "metadata": {
        "id": "OdNx_PzWPCys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A is singular, so the outputs above should not be a surprise. Below is a non-singular matrix with its square and inverse respectively:"
      ],
      "metadata": {
        "id": "HV1QSARHPIhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, -1, 3],[ 3, 5, 1],[7, 0, 9]])\n",
        "print(A)\n",
        "\n",
        "B = linalg.matrix_power(A,2)\n",
        "C = linalg.inv(A)\n",
        "D = linalg.inv(C) #Allowing rounding-off, D=A\n",
        "e = linalg.det(A)\n",
        "\n",
        "print(\"Matrix's determinant is: \",e)\n",
        "\n",
        "print(B)\n",
        "print(C)\n",
        "print(D)"
      ],
      "metadata": {
        "id": "OOAocJSEPJWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvectors and eigenvalues#\n",
        "If we recall, an eigenvector v of a square matrix A is defined as:\n",
        "\n",
        "Av=\\lambda v\n",
        "Av=λv\n",
        "\n",
        "where λ is the corresponding eigenvalue.\n",
        "\n",
        "lamba_values, eigen_vect = linalg.eig(A)\n",
        "\n",
        "Compared to normal NumPy, the JAX version treats every value as complex, so don’t be surprised by the j we observe in the output values.\n",
        "\n",
        "It may be tempting to use “lambda” as a variable identifier. Don’t do this since lambda is a keyword. Instead, we can use λ itself as an identifier.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHSRz4_QPLjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[2,1,5],[3,0,4],[2,1,-2]])\n",
        "\n",
        "λ, eigen_vectors = linalg.eig(A)\n",
        "\n",
        "print(\"The λ values are:,\",λ)\n",
        "print(\"And Eigen vectors (as a matrix) are:\",eigen_vectors)"
      ],
      "metadata": {
        "id": "CVDwFwk2PQjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definiteness#**\n",
        "Eigenvalues of a vector introduce another related concept.\n",
        "\n",
        "We call a square matrix a positive definite matrix if all the eigenvalues are positive while a positive semidefinite matrix is one having all its eigenvalues either zero or positive.\n",
        "\n",
        "We can extend this concept to negative definite and semidefinite matrices as well.\n",
        "\n",
        "If a matrix has some positive eigenvalues and other negatives, it’s called an indefinite matrix. These matrices are pretty helpful as we will shortly see in the follow-up lessons.\n",
        "\n",
        "Note: The concept of definiteness is usually restricted to only real-value square matrices, but can be expanded to complex matrices if needed."
      ],
      "metadata": {
        "id": "oN4CvlxEPUrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TestDefiniteness(X):\n",
        "  if(jnp.all(linalg.eigvals(X) > 0)):\n",
        "    print(\"It's Positive Definite\")\n",
        "  elif(jnp.all(linalg.eigvals(X) >= 0)):\n",
        "    print(\"It's Positive Semidefinite\")\n",
        "  elif(jnp.all(linalg.eigvals(X) < 0)):\n",
        "    print(\"It's Negative Definite\")\n",
        "  elif(jnp.all(linalg.eigvals(X) <= 0)):\n",
        "    print(\"It's Negative Semidefinite\")\n",
        "  else:\n",
        "    print(\"Its Indefinite\")\n",
        "\n",
        "#Lets test some matrices\n",
        "\n",
        "A = jnp.array(([2.0,3.0,4.0],[1.2,2.4,5.2],[2.8,3.6,4.9]))\n",
        "B = jnp.array(([2.4,4.3],[6.1, 13.6]))\n",
        "TestDefiniteness(A)\n",
        "TestDefiniteness(B)"
      ],
      "metadata": {
        "id": "qVZw_3I8PWkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Singular Value Decomposition#\n",
        "We’ll conclude the lesson by performing a singular value decomposition (SVD) on a given matrix. If you recall, an SVD of matrix A is:\n",
        "\n",
        "U~\\Sigma ~V^* = A\n",
        "U Σ V \n",
        "∗\n",
        " =A\n",
        "\n",
        "If A is m\\times n\n",
        "m×n\n",
        ", then the dimensions of the decomposed matrices are:\n",
        "\n",
        "U: m \\times m\n",
        "U:m×m\n",
        "\n",
        "\\Sigma: m \\times n\n",
        "Σ:m×n\n",
        "\n",
        "V^*: n \\times n\n",
        "V \n",
        "∗\n",
        " :n×n\n",
        "\n",
        "We can verify the dimensions in this example:"
      ],
      "metadata": {
        "id": "Qlvz3ukiPasi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[2,1,5],[3,0,4],[2,1,-2],[4,0,7]])\n",
        "\n",
        "U, Sigma, V = linalg.svd(A)\n",
        "\n",
        "print(\"Original matrix shape:\",A.shape)\n",
        "print(\"U shape:\",U.shape)\n",
        "print(\"Σ shape:\",Sigma.shape)\n",
        "print(\"V* shape:\", V.shape)"
      ],
      "metadata": {
        "id": "9qAh2G_TPdaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look closely, while the shapes of U\n",
        "U\n",
        " and V^*\n",
        "V \n",
        "∗\n",
        " \n",
        " are consistent with the formula, \\Sigma\n",
        "Σ\n",
        " is returned as a one-dimensional vector of singular values, which is consistent with NumPy."
      ],
      "metadata": {
        "id": "c2QqVQ3sPgH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BozjWJS-Phpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}